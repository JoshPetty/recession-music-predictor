{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c1980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==================== CONFIGURATION ====================\n",
    "# Update these paths to match your files\n",
    "TOPICS_FILE = r'C:\\Users\\joshu\\OneDrive\\Desktop\\CS74\\Final_Project\\data\\weekly_topic_distributions.csv'\n",
    "ECON_FILE = r'C:\\Users\\joshu\\OneDrive\\Desktop\\CS74\\Final_Project\\music_econ_merged.csv'\n",
    "OUTPUT_FILE = r'C:\\Users\\joshu\\OneDrive\\Desktop\\CS74\\Final_Project\\music_econ_topics_merged.csv'\n",
    "\n",
    "# ==================== LOAD DATA ====================\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "topics_weekly = pd.read_csv(TOPICS_FILE)\n",
    "econ_df = pd.read_csv(ECON_FILE)\n",
    "\n",
    "print(f\"Weekly topics loaded: {len(topics_weekly)} rows\")\n",
    "print(f\"Columns: {list(topics_weekly.columns)[:5]}...\")\n",
    "\n",
    "print(f\"\\nEconomic data loaded: {len(econ_df)} rows\")\n",
    "\n",
    "# Convert dates\n",
    "topics_weekly['week_date'] = pd.to_datetime(topics_weekly['week_date'])\n",
    "econ_df['week_date'] = pd.to_datetime(econ_df['week_date'])\n",
    "\n",
    "print(f\"\\nWeekly topics date range: {topics_weekly['week_date'].min()} to {topics_weekly['week_date'].max()}\")\n",
    "print(f\"Economic data date range: {econ_df['week_date'].min()} to {econ_df['week_date'].max()}\")\n",
    "\n",
    "# Topic columns are everything except 'week_date'\n",
    "all_cols = list(topics_weekly.columns)\n",
    "topic_cols = [col for col in all_cols if col != 'week_date']\n",
    "\n",
    "print(f\"\\nFound {len(topic_cols)} topic columns\")\n",
    "print(f\"Topic columns: {topic_cols}\")\n",
    "\n",
    "# ==================== AGGREGATE TO MONTHLY ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"AGGREGATING WEEKLY TO MONTHLY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract year and month for grouping\n",
    "topics_weekly['year'] = topics_weekly['week_date'].dt.year\n",
    "topics_weekly['month'] = topics_weekly['week_date'].dt.month\n",
    "\n",
    "# Average probabilities by year-month\n",
    "topics_monthly = topics_weekly.groupby(['year', 'month'])[topic_cols].mean().reset_index()\n",
    "\n",
    "# Create month-end dates to match economic data format\n",
    "topics_monthly['week_date'] = pd.to_datetime(\n",
    "    topics_monthly['year'].astype(str) + '-' + \n",
    "    topics_monthly['month'].astype(str).str.zfill(2) + '-01'\n",
    ") + pd.offsets.MonthEnd(0)\n",
    "\n",
    "# Drop year/month columns\n",
    "topics_monthly = topics_monthly.drop(['year', 'month'], axis=1)\n",
    "\n",
    "print(f\"✓ Aggregated to {len(topics_monthly)} months\")\n",
    "print(f\"Date range: {topics_monthly['week_date'].min()} to {topics_monthly['week_date'].max()}\")\n",
    "\n",
    "# ==================== CALCULATE DOMINANT TOPICS ====================\n",
    "\n",
    "# Find dominant topic (highest probability)\n",
    "topics_monthly['dominant_topic'] = topics_monthly[topic_cols].idxmax(axis=1)\n",
    "topics_monthly['dominant_prob'] = topics_monthly[topic_cols].max(axis=1)\n",
    "\n",
    "# Find second dominant\n",
    "topics_monthly['second_topic'] = topics_monthly[topic_cols].apply(\n",
    "    lambda row: row.nlargest(2).index[-1] if len(row) >= 2 else None, \n",
    "    axis=1\n",
    ")\n",
    "topics_monthly['second_prob'] = topics_monthly[topic_cols].apply(\n",
    "    lambda row: row.nlargest(2).iloc[-1] if len(row) >= 2 else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"\\nDominant topic distribution:\")\n",
    "print(topics_monthly['dominant_topic'].value_counts().sort_index())\n",
    "\n",
    "# ==================== MERGE WITH ECONOMIC DATA ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MERGING WITH ECONOMIC DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "merged_df = pd.merge(econ_df, topics_monthly, on='week_date', how='left')\n",
    "\n",
    "print(f\"\\nOriginal econ data: {len(econ_df)} rows\")\n",
    "print(f\"Monthly topic data: {len(topics_monthly)} rows\")\n",
    "print(f\"Merged data: {len(merged_df)} rows\")\n",
    "print(f\"Rows with topics: {merged_df['dominant_topic'].notna().sum()}\")\n",
    "print(f\"Rows missing topics: {merged_df['dominant_topic'].isna().sum()}\")\n",
    "\n",
    "if merged_df['dominant_topic'].isna().sum() > 0:\n",
    "    print(\"\\n⚠️  Some rows missing topic data:\")\n",
    "    missing = merged_df[merged_df['dominant_topic'].isna()][['week_date']]\n",
    "    print(f\"   First missing date: {missing['week_date'].min()}\")\n",
    "    print(f\"   Last missing date: {missing['week_date'].max()}\")\n",
    "    print(\"   (Topic data may not cover full date range)\")\n",
    "\n",
    "# ==================== ADD HUMAN-READABLE LABELS ====================\n",
    "\n",
    "topic_labels = {\n",
    "    '0': \"Spiritual\", '1': \"Relationshp\", '2': \"Longing\",\n",
    "    '3': \"Clarity\", '4': \"Everyday joy\", '5': \"Dance,Tonight's a Movie\",\n",
    "    '6': \"Heartbreak\", '7': \"Intimacy\", '8': \"Self/Body\",\n",
    "    '9': \"Party/Club\", '10': \"Bop/Dance vibes\", '11': \"Social Talk\",\n",
    "    '12': \"Girl & Fun\", '13': \"Music & Friends\", '14': \"Time, Thinking, & Change\",\n",
    "    '15': \"Appreciation\", '16': \"Admiring\", '17': \"Regret/Reflection\",\n",
    "    '18': \"Past/Memory\", '19': \"Hopes & Dreams\", '20': \"True Love\",\n",
    "    '21': \"Money/Power & Social Commentary\", '22': \"Romatinc/Intimate\", '23': \"Hip-Hop/Urban\",\n",
    "    '24': \"Home\", '25': \"Dance & Music\", '26': \"Playfullness\",\n",
    "    '27': \"Feeling Good\", '28': \"Freaky-Deaky\", '29': \"Romantic yearning\"\n",
    "}\n",
    "\n",
    "merged_df['dominant_topic_label'] = merged_df['dominant_topic'].map(topic_labels)\n",
    "merged_df['second_topic_label'] = merged_df['second_topic'].map(topic_labels)\n",
    "\n",
    "# ==================== SHOW SAMPLE ====================\n",
    "print(\"\\n--- SAMPLE OF MERGED DATA ---\")\n",
    "sample_cols = ['week_date', 'USREC', 'acousticness', 'dominant_topic', \n",
    "               'dominant_topic_label', 'dominant_prob']\n",
    "\n",
    "if all(col in merged_df.columns for col in sample_cols):\n",
    "    print(merged_df[sample_cols].head(15).to_string(index=False))\n",
    "\n",
    "# ==================== SAVE ====================\n",
    "merged_df.to_csv(OUTPUT_FILE, index=False)\n",
    "print(f\"\\n✓ Saved: {OUTPUT_FILE}\")\n",
    "print(f\"Total columns: {len(merged_df.columns)}\")\n",
    "\n",
    "# ==================== ANALYZE RECESSION VS NORMAL ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOPIC ANALYSIS: RECESSION VS NORMAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Filter to rows with topic data\n",
    "df_with_topics = merged_df[merged_df['dominant_topic'].notna()].copy()\n",
    "\n",
    "print(f\"\\nAnalyzing {len(df_with_topics)} months with topic data\")\n",
    "\n",
    "if 'USREC' in df_with_topics.columns:\n",
    "    recession_count = (df_with_topics['USREC'] == 1).sum()\n",
    "    normal_count = (df_with_topics['USREC'] == 0).sum()\n",
    "    \n",
    "    print(f\"  Recession months: {recession_count}\")\n",
    "    print(f\"  Normal months: {normal_count}\")\n",
    "    \n",
    "    if recession_count > 0 and normal_count > 0:\n",
    "        # Calculate topic distributions\n",
    "        rec_topics = df_with_topics[df_with_topics['USREC'] == 1]['dominant_topic_label'].value_counts(normalize=True) * 100\n",
    "        norm_topics = df_with_topics[df_with_topics['USREC'] == 0]['dominant_topic_label'].value_counts(normalize=True) * 100\n",
    "        \n",
    "        print(\"\\n--- TOP 5 TOPICS DURING RECESSIONS ---\")\n",
    "        for topic, pct in rec_topics.head(5).items():\n",
    "            print(f\"  {topic:25s}: {pct:5.1f}%\")\n",
    "        \n",
    "        print(\"\\n--- TOP 5 TOPICS DURING NORMAL TIMES ---\")\n",
    "        for topic, pct in norm_topics.head(5).items():\n",
    "            print(f\"  {topic:25s}: {pct:5.1f}%\")\n",
    "        \n",
    "        # Calculate differences\n",
    "        all_topics = set(rec_topics.index) | set(norm_topics.index)\n",
    "        differences = []\n",
    "        \n",
    "        for topic in all_topics:\n",
    "            rec_pct = rec_topics.get(topic, 0)\n",
    "            norm_pct = norm_topics.get(topic, 0)\n",
    "            differences.append({\n",
    "                'Topic': topic,\n",
    "                'Recession %': rec_pct,\n",
    "                'Normal %': norm_pct,\n",
    "                'Difference': rec_pct - norm_pct\n",
    "            })\n",
    "        \n",
    "        diff_df = pd.DataFrame(differences).sort_values('Difference', ascending=False)\n",
    "        \n",
    "        print(\"\\n--- BIGGEST INCREASES DURING RECESSIONS ---\")\n",
    "        for _, row in diff_df.head(5).iterrows():\n",
    "            print(f\"  {row['Topic']:25s}: {row['Recession %']:5.1f}% vs {row['Normal %']:5.1f}% (Δ {row['Difference']:+5.1f}%)\")\n",
    "        \n",
    "        print(\"\\n--- BIGGEST DECREASES DURING RECESSIONS ---\")\n",
    "        for _, row in diff_df.tail(5).iterrows():\n",
    "            print(f\"  {row['Topic']:25s}: {row['Recession %']:5.1f}% vs {row['Normal %']:5.1f}% (Δ {row['Difference']:+5.1f}%)\")\n",
    "\n",
    "# ==================== SUMMARY ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY FOR MODELING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nYour merged dataset has {len(merged_df.columns)} columns:\")\n",
    "print(f\"  - Original economic/audio features\")\n",
    "print(f\"  - {len(topic_cols)} topic probability columns (0-29)\")\n",
    "print(f\"  - 4 new dominant topic columns\")\n",
    "\n",
    "print(f\"\\nTo use all features in your model:\")\n",
    "print(f\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define features\n",
    "audio_features = ['danceability', 'energy', 'valence', 'tempo', \n",
    "                  'acousticness', 'instrumentalness', 'speechiness', \n",
    "                  'loudness', 'pos', 'neg']\n",
    "topic_features = {topic_cols}\n",
    "\n",
    "all_features = audio_features + topic_features\n",
    "\n",
    "# Prepare data\n",
    "df = pd.read_csv('{OUTPUT_FILE}')\n",
    "df = df.dropna(subset=all_features + ['USREC'])  # Remove rows with missing data\n",
    "\n",
    "X = df[all_features]\n",
    "y = df['USREC']\n",
    "\n",
    "# Split and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✓ Done!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
